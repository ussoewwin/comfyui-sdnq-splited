"""
SDNQ Standalone Sampler Node - V3 API Compatible

This node loads SDNQ quantized models and generates images in one step.
No MODEL/CLIP/VAE outputs - just IMAGE output for ComfyUI.

Architecture: User Input → Load/Download SDNQ Model → Generate → Output IMAGE

Based on verified APIs from:
- diffusers documentation (https://huggingface.co/docs/diffusers)
- SDNQ repository (https://github.com/Disty0/sdnq)
- ComfyUI nodes.py (IMAGE format specification)
"""

import torch
import numpy as np
from PIL import Image
import traceback
import sys
import os
from typing import Optional, Tuple, Dict, Any

# ComfyUI imports for LoRA folder access
try:
    import folder_paths
    COMFYUI_AVAILABLE = True
except ImportError:
    COMFYUI_AVAILABLE = False
    print("[SDNQ Sampler] Warning: folder_paths not available - LoRA dropdown will be disabled")

# SDNQ import - registers SDNQ support into diffusers
from sdnq import SDNQConfig

# diffusers pipeline - auto-detects model type from model_index.json
from diffusers import DiffusionPipeline

# Scheduler imports
# Flow-match schedulers (for FLUX, SD3, Qwen, Z-Image)
from diffusers.schedulers import FlowMatchEulerDiscreteScheduler

# Traditional diffusion schedulers (for SDXL, SD1.5, etc.)
from diffusers.schedulers import (
    DDIMScheduler,
    DDPMScheduler,
    PNDMScheduler,
    LMSDiscreteScheduler,
    EulerDiscreteScheduler,
    HeunDiscreteScheduler,
    EulerAncestralDiscreteScheduler,
    DPMSolverMultistepScheduler,
    DPMSolverSinglestepScheduler,
    KDPM2DiscreteScheduler,
    KDPM2AncestralDiscreteScheduler,
    DEISMultistepScheduler,
    UniPCMultistepScheduler,
)

# Local imports for model catalog and downloading
from ..core.registry import (
    get_model_names_for_dropdown,
    get_repo_id_from_name,
    get_model_info,
)
from ..core.downloader import (
    download_model,
    get_cached_model_path,
    check_model_cached,
)
from ..core.config import get_sdnq_models_dir


class SDNQSampler:
    """
    Standalone SDNQ sampler that loads quantized models and generates images.

    All-in-one node that handles:
    - Model selection from pre-configured catalog OR custom paths
    - Auto-download from HuggingFace Hub
    - Loading SDNQ models from local paths
    - Setting up generation parameters
    - Generating images with proper seeding
    - Converting output to ComfyUI IMAGE format
    - Graceful error handling and interruption support

    ComfyUI V3 API Compatible with V1 backward compatibility.
    """

    def __init__(self):
        """Initialize sampler with empty pipeline cache."""
        self.pipeline = None
        self.current_model_path = None
        self.current_dtype = None
        self.current_memory_mode = None
        self.current_scheduler = None
        self.current_lora_path = None
        self.current_lora_strength = None
        self.interrupted = False

    @classmethod
    def INPUT_TYPES(cls):
        """
        Define node inputs following ComfyUI V3 conventions with V1 compatibility.

        Parameters organized logically:
        1. Model Selection (what to use)
        2. Generation Prompts (what to create)
        3. Generation Settings (how to create)
        4. Model Configuration (technical settings)
        5. Enhancements (optional improvements)

        All parameters verified from diffusers documentation.
        """
        # Get model names from catalog
        model_names = get_model_names_for_dropdown()

        # Get available LoRAs from ComfyUI loras folder
        lora_list = ["[None]", "[Custom Path]"]
        if COMFYUI_AVAILABLE:
            try:
                available_loras = folder_paths.get_filename_list("loras")
                lora_list.extend(available_loras)
            except Exception as e:
                print(f"[SDNQ Sampler] Warning: Could not load LoRA list: {e}")

        # Scheduler options
        # Flow-based schedulers ONLY work with FLUX/SD3/Qwen/Z-Image models
        # Traditional schedulers ONLY work with SDXL/SD1.5 models
        scheduler_list = [
            # Flow-based (for FLUX, SD3, Qwen, Z-Image)
            "FlowMatchEulerDiscreteScheduler",
            # Traditional diffusion (for SDXL, SD1.5) - Top 10 most popular
            "DPMSolverMultistepScheduler",
            "UniPCMultistepScheduler",
            "EulerDiscreteScheduler",
            "EulerAncestralDiscreteScheduler",
            "DDIMScheduler",
            "HeunDiscreteScheduler",
            "KDPM2DiscreteScheduler",
            "KDPM2AncestralDiscreteScheduler",
            "DPMSolverSinglestepScheduler",
            "DEISMultistepScheduler",
            "LMSDiscreteScheduler",
            "DDPMScheduler",
            "PNDMScheduler",
        ]

        return {
            "required": {
                # ============================================================
                # GROUP 1: MODEL SELECTION (What to use)
                # ============================================================

                "model_selection": (["[Custom Path]"] + model_names, {
                    "default": model_names[0] if model_names else "[Custom Path]",
                    "tooltip": "Select a pre-configured SDNQ model (auto-downloads from HuggingFace) or choose [Custom Path] to specify a local model directory"
                }),

                "custom_model_path": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "Local path to SDNQ model directory (only used when [Custom Path] is selected). Example: /path/to/model or C:\\path\\to\\model"
                }),

                # ============================================================
                # GROUP 2: GENERATION PROMPTS (What to create)
                # ============================================================

                "prompt": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "tooltip": "Text description of the image to generate. Be descriptive for best results."
                }),

                "negative_prompt": ("STRING", {
                    "default": "blurry, low quality, distorted, deformed, ugly, bad anatomy, bad hands, text, watermark, signature",
                    "multiline": True,
                    "tooltip": "What to avoid in the image. Default includes common quality issues. Clear this for no negative prompt. Note: FLUX-schnell (cfg=0) ignores negative prompts."
                }),

                # ============================================================
                # GROUP 3: GENERATION SETTINGS (How to create)
                # ============================================================

                "steps": ("INT", {
                    "default": 25,
                    "min": 1,
                    "max": 150,
                    "step": 1,
                    "tooltip": "Number of denoising steps. More steps = better quality but slower. 20-30 is typical for most models."
                }),

                "cfg": ("FLOAT", {
                    "default": 7.0,
                    "min": 0.0,
                    "max": 30.0,
                    "step": 0.1,
                    "tooltip": "Guidance scale - how closely to follow the prompt. Higher = more literal. FLUX-schnell uses 0.0, FLUX-dev uses 3.5-7.0, SDXL uses 7.0-9.0."
                }),

                "width": ("INT", {
                    "default": 1024,
                    "min": 64,
                    "max": 4096,
                    "step": 8,
                    "tooltip": "Image width in pixels. Must be multiple of 8. Larger = more VRAM usage. 1024 is standard for FLUX/SDXL."
                }),

                "height": ("INT", {
                    "default": 1024,
                    "min": 64,
                    "max": 4096,
                    "step": 8,
                    "tooltip": "Image height in pixels. Must be multiple of 8. Larger = more VRAM usage. 1024 is standard for FLUX/SDXL."
                }),

                "seed": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 0xffffffffffffffff,
                    "tooltip": "Random seed for reproducible generation. Same seed + settings = same image."
                }),

                "scheduler": (scheduler_list, {
                    "default": "DPMSolverMultistepScheduler",
                    "tooltip": "⚠️ IMPORTANT: Use FlowMatchEulerDiscreteScheduler for FLUX/SD3/Qwen/Z-Image. Use DPMSolver/Euler/UniPC for SDXL/SD1.5. Wrong scheduler = broken images!"
                }),

                # ============================================================
                # GROUP 4: MODEL CONFIGURATION (Technical settings)
                # ============================================================

                "dtype": (["bfloat16", "float16", "float32"], {
                    "default": "bfloat16",
                    "tooltip": "Model precision. bfloat16 recommended for FLUX (best quality/speed). float16 for older GPUs. float32 for CPU."
                }),

                "memory_mode": (["gpu", "balanced", "lowvram"], {
                    "default": "balanced",
                    "tooltip": "Memory management: 'gpu' = All on GPU (fastest, needs 24GB+ VRAM). 'balanced' = Model offloading (12-16GB VRAM). 'lowvram' = Sequential offloading (8GB VRAM, slowest)."
                }),

                "auto_download": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Automatically download model from HuggingFace if not found locally. Disable to only use local models."
                }),
            },
            "optional": {
                # ============================================================
                # GROUP 5: ENHANCEMENTS (Optional improvements)
                # ============================================================

                "lora_selection": (lora_list, {
                    "default": "[None]",
                    "tooltip": "Select LoRA from ComfyUI loras folder ([None] = disabled, [Custom Path] = use custom path below). LoRAs add styles or concepts to generation."
                }),

                "lora_custom_path": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "Custom LoRA path or HuggingFace repo ID (only used when [Custom Path] is selected). Example: /path/to/lora.safetensors or username/lora-repo"
                }),

                "lora_strength": ("FLOAT", {
                    "default": 1.0,
                    "min": -5.0,
                    "max": 5.0,
                    "step": 0.05,
                    "tooltip": "LoRA influence strength. 1.0 = full strength, 0.5 = half strength, 0.0 = disabled. Negative values invert the LoRA effect. Range: -5.0 to +5.0."
                }),
            }
        }

    # V3 API: Return type hints
    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image",)

    # V1 API: Function name
    FUNCTION = "generate"

    # Category for node menu
    CATEGORY = "sampling/SDNQ"

    # V3 API: Output node (can save/display results)
    OUTPUT_NODE = False

    # V3 API: Node description
    DESCRIPTION = "Load SDNQ quantized models and generate images with 50-75% VRAM savings. Supports FLUX, SD3, SDXL, video models, and more."

    def check_interrupted(self):
        """Check if generation should be interrupted (ComfyUI interrupt support)."""
        # ComfyUI provides comfy.model_management.interrupt_processing()
        # For now, we'll use a simple flag that can be extended
        return self.interrupted

    def load_or_download_model(self, model_selection: str, custom_path: str, auto_download: bool) -> Tuple[str, bool]:
        """
        Load model from catalog or custom path, downloading if needed.

        Args:
            model_selection: Selected model from dropdown
            custom_path: Custom model path (if [Custom Path] selected)
            auto_download: Whether to auto-download from HuggingFace

        Returns:
            Tuple of (model_path: str, was_downloaded: bool)

        Raises:
            ValueError: If model not found and auto_download is False
            Exception: If download fails
        """
        # Check if using custom path
        if model_selection == "[Custom Path]":
            if not custom_path or custom_path.strip() == "":
                raise ValueError(
                    "Custom model path is empty. Please provide a valid path to a local SDNQ model directory, "
                    "or select a pre-configured model from the dropdown."
                )

            model_path = custom_path.strip()

            # Verify path exists
            import os
            if not os.path.exists(model_path):
                raise FileNotFoundError(
                    f"Custom model path does not exist: {model_path}\n"
                    f"Please verify the path and try again."
                )

            # Verify it's a valid model directory (has model_index.json)
            if not os.path.exists(os.path.join(model_path, "model_index.json")):
                raise ValueError(
                    f"Invalid model directory: {model_path}\n"
                    f"Directory must contain model_index.json file. "
                    f"This should be a diffusers model directory."
                )

            print(f"[SDNQ Sampler] Using custom model path: {model_path}")
            return (model_path, False)

        # Using catalog model
        model_info = get_model_info(model_selection)
        if not model_info:
            raise ValueError(
                f"Model not found in catalog: {model_selection}\n"
                f"This may indicate an issue with the model registry. "
                f"Try selecting a different model or using [Custom Path]."
            )

        repo_id = model_info["repo_id"]
        print(f"[SDNQ Sampler] Selected model: {model_selection}")
        print(f"[SDNQ Sampler] Repository: {repo_id}")

        # Check if model already cached
        cached_path = get_cached_model_path(repo_id)
        if cached_path:
            print(f"[SDNQ Sampler] Found cached model at: {cached_path}")
            return (cached_path, False)

        # Model not cached - download if auto_download enabled
        if not auto_download:
            raise ValueError(
                f"Model not found locally: {model_selection} ({repo_id})\n\n"
                f"Options:\n"
                f"1. Enable 'auto_download' to download automatically from HuggingFace\n"
                f"2. Download manually using: huggingface-cli download {repo_id}\n"
                f"3. Select a different model that's already downloaded"
            )

        print(f"[SDNQ Sampler] Model not cached - downloading from HuggingFace...")
        print(f"[SDNQ Sampler] This may take a while (models are 5-20+ GB)")

        try:
            downloaded_path = download_model(repo_id)
            print(f"[SDNQ Sampler] Download complete: {downloaded_path}")
            return (downloaded_path, True)
        except Exception as e:
            raise Exception(
                f"Failed to download model {model_selection} ({repo_id})\n\n"
                f"Error: {str(e)}\n\n"
                f"Troubleshooting:\n"
                f"1. Check your internet connection\n"
                f"2. Verify HuggingFace Hub is accessible\n"
                f"3. Try downloading manually: huggingface-cli download {repo_id}\n"
                f"4. Check disk space (models are large)\n"
                f"5. If download was interrupted, try again - it will resume"
            )

    def load_pipeline(self, model_path: str, dtype_str: str, memory_mode: str = "gpu") -> DiffusionPipeline:
        """
        Load SDNQ model using diffusers pipeline.

        Uses DiffusionPipeline which auto-detects the correct pipeline class
        from the model's model_index.json file. This works with:
        - FLUX.1, FLUX.2
        - SD3, SD3.5, SDXL
        - Video models (CogVideoX, Wan, etc.)
        - Multimodal models (Z-Image, Qwen-Image, etc.)

        Args:
            model_path: Local path to SDNQ model directory
            dtype_str: String dtype ("bfloat16", "float16", "float32")
            memory_mode: Memory management strategy ("gpu", "balanced", "lowvram")

        Returns:
            Loaded diffusers pipeline

        Raises:
            Exception: If pipeline loading fails

        Based on verified API from:
        https://huggingface.co/docs/diffusers/en/using-diffusers/loading
        https://huggingface.co/docs/diffusers/main/optimization/memory
        """
        # Convert dtype string to torch dtype
        dtype_map = {
            "bfloat16": torch.bfloat16,
            "float16": torch.float16,
            "float32": torch.float32,
        }
        torch_dtype = dtype_map[dtype_str]

        print(f"[SDNQ Sampler] Loading model from: {model_path}")
        print(f"[SDNQ Sampler] Using dtype: {dtype_str} ({torch_dtype})")
        print(f"[SDNQ Sampler] Memory mode: {memory_mode}")

        try:
            # Load pipeline - DiffusionPipeline auto-detects model type
            # SDNQ quantization is automatically detected from model config
            # Note: Pipeline loads to CPU by default - we move to GPU below
            pipeline = DiffusionPipeline.from_pretrained(
                model_path,
                torch_dtype=torch_dtype,
                local_files_only=True,  # Only load from local path
            )

            # Apply memory management strategy
            # Based on: https://huggingface.co/docs/diffusers/main/optimization/memory
            if memory_mode == "gpu":
                # Full GPU mode: Fastest performance, needs 24GB+ VRAM
                # Load entire pipeline to GPU
                print(f"[SDNQ Sampler] Moving model to GPU (full GPU mode)...")
                pipeline.to("cuda")
                print(f"[SDNQ Sampler] ✓ Model loaded to GPU (all components on VRAM)")

            elif memory_mode == "balanced":
                # Model CPU offload: Good balance for 12-16GB VRAM
                # Moves whole models between CPU and GPU as needed
                print(f"[SDNQ Sampler] Enabling model CPU offload (balanced mode)...")
                pipeline.enable_model_cpu_offload()
                print(f"[SDNQ Sampler] ✓ Model offloading enabled (efficient VRAM usage)")

            elif memory_mode == "lowvram":
                # Sequential CPU offload: Maximum memory savings for 8GB VRAM
                # Slowest but works on limited VRAM
                print(f"[SDNQ Sampler] Enabling sequential CPU offload (low VRAM mode)...")
                pipeline.enable_sequential_cpu_offload()
                print(f"[SDNQ Sampler] ✓ Sequential offloading enabled (minimal VRAM usage)")

            print(f"[SDNQ Sampler] Model loaded successfully!")
            print(f"[SDNQ Sampler] Pipeline type: {type(pipeline).__name__}")

            return pipeline

        except Exception as e:
            raise Exception(
                f"Failed to load SDNQ model from: {model_path}\n\n"
                f"Error: {str(e)}\n\n"
                f"Troubleshooting:\n"
                f"1. Verify the path contains a valid diffusers model (should have model_index.json)\n"
                f"2. Check if model download completed successfully\n"
                f"3. Try a different dtype (bfloat16 requires modern GPUs)\n"
                f"4. Check VRAM availability (use smaller model if needed)\n"
                f"5. Look at the error message above for specific details"
            )

    def load_lora(self, pipeline: DiffusionPipeline, lora_path: str, lora_strength: float = 1.0):
        """
        Load LoRA weights into pipeline.

        Supports both local .safetensors files and HuggingFace repo IDs.

        Args:
            pipeline: Loaded diffusers pipeline
            lora_path: Path to LoRA file or HuggingFace repo ID
            lora_strength: LoRA influence strength (0.0 to 2.0)

        Based on verified API from:
        https://huggingface.co/docs/diffusers/en/api/loaders/lora
        https://huggingface.co/blog/lora-fast
        """
        import os

        if not lora_path or lora_path.strip() == "":
            print(f"[SDNQ Sampler] No LoRA specified, skipping...")
            return

        print(f"[SDNQ Sampler] Loading LoRA...")
        print(f"[SDNQ Sampler]   Path: {lora_path}")
        print(f"[SDNQ Sampler]   Strength: {lora_strength}")

        try:
            # Check if it's a local file or HuggingFace repo
            is_local_file = os.path.exists(lora_path) and os.path.isfile(lora_path)

            if is_local_file:
                # Local .safetensors file
                # Extract directory and filename
                lora_dir = os.path.dirname(lora_path)
                lora_file = os.path.basename(lora_path)

                pipeline.load_lora_weights(
                    lora_dir,
                    weight_name=lora_file,
                    adapter_name="lora"
                )
            else:
                # Assume it's a HuggingFace repo ID
                pipeline.load_lora_weights(
                    lora_path,
                    adapter_name="lora"
                )

            # Set LoRA strength
            if lora_strength != 1.0:
                pipeline.set_adapters(["lora"], adapter_weights=[lora_strength])
            else:
                pipeline.set_adapters(["lora"])

            print(f"[SDNQ Sampler] ✓ LoRA loaded successfully")

        except Exception as e:
            raise Exception(
                f"Failed to load LoRA\n\n"
                f"Error: {str(e)}\n\n"
                f"LoRA path: {lora_path}\n\n"
                f"Troubleshooting:\n"
                f"1. Verify LoRA file exists (.safetensors format)\n"
                f"2. For HuggingFace repos, verify repo ID is correct\n"
                f"3. Ensure LoRA is compatible with the model architecture\n"
                f"4. Check if LoRA is for the correct model type (FLUX, SDXL, etc.)\n"
                f"5. Try with lora_strength=1.0 first"
            )

    def unload_lora(self, pipeline: DiffusionPipeline):
        """
        Unload LoRA weights from pipeline.

        Args:
            pipeline: Pipeline with loaded LoRA
        """
        try:
            if hasattr(pipeline, 'unload_lora_weights'):
                print(f"[SDNQ Sampler] Unloading previous LoRA...")
                pipeline.unload_lora_weights()
        except Exception as e:
            # Non-critical error, just log it
            print(f"[SDNQ Sampler] Warning: Failed to unload LoRA: {e}")

    def swap_scheduler(self, pipeline: DiffusionPipeline, scheduler_name: str):
        """
        Swap the pipeline's scheduler.

        Uses the from_config() pattern to create a new scheduler with the same
        configuration as the current scheduler but with different algorithm.

        Args:
            pipeline: Loaded diffusers pipeline
            scheduler_name: Name of scheduler to use

        Raises:
            Exception: If scheduler swap fails

        Based on verified API from:
        https://huggingface.co/docs/diffusers/en/using-diffusers/schedulers
        """
        print(f"[SDNQ Sampler] Swapping scheduler to: {scheduler_name}")

        try:
            # Map scheduler name to class
            scheduler_map = {
                # Flow-based schedulers (FLUX, SD3, Qwen, Z-Image)
                "FlowMatchEulerDiscreteScheduler": FlowMatchEulerDiscreteScheduler,
                # Traditional diffusion schedulers (SDXL, SD1.5)
                "DPMSolverMultistepScheduler": DPMSolverMultistepScheduler,
                "UniPCMultistepScheduler": UniPCMultistepScheduler,
                "EulerDiscreteScheduler": EulerDiscreteScheduler,
                "EulerAncestralDiscreteScheduler": EulerAncestralDiscreteScheduler,
                "DDIMScheduler": DDIMScheduler,
                "HeunDiscreteScheduler": HeunDiscreteScheduler,
                "KDPM2DiscreteScheduler": KDPM2DiscreteScheduler,
                "KDPM2AncestralDiscreteScheduler": KDPM2AncestralDiscreteScheduler,
                "DPMSolverSinglestepScheduler": DPMSolverSinglestepScheduler,
                "DEISMultistepScheduler": DEISMultistepScheduler,
                "LMSDiscreteScheduler": LMSDiscreteScheduler,
                "DDPMScheduler": DDPMScheduler,
                "PNDMScheduler": PNDMScheduler,
            }

            if scheduler_name not in scheduler_map:
                raise ValueError(
                    f"Unknown scheduler: {scheduler_name}\n"
                    f"Available schedulers: {list(scheduler_map.keys())}"
                )

            scheduler_class = scheduler_map[scheduler_name]

            # Swap scheduler using from_config pattern
            # This preserves scheduler configuration while changing the algorithm
            pipeline.scheduler = scheduler_class.from_config(pipeline.scheduler.config)

            print(f"[SDNQ Sampler] ✓ Scheduler swapped successfully")

        except Exception as e:
            raise Exception(
                f"Failed to swap scheduler\n\n"
                f"Error: {str(e)}\n\n"
                f"Requested scheduler: {scheduler_name}\n\n"
                f"Troubleshooting:\n"
                f"1. Ensure scheduler is compatible with the model type\n"
                f"2. FLUX/SD3/Qwen/Z-Image: Use FlowMatchEulerDiscreteScheduler\n"
                f"3. SDXL/SD1.5: Use DPMSolver, Euler, UniPC, or DDIM\n"
                f"4. Wrong scheduler type will produce broken/corrupted images\n"
                f"5. Check diffusers version (requires >=0.36.0)"
            )

    def generate_image(self, pipeline: DiffusionPipeline, prompt: str, negative_prompt: str,
                      steps: int, cfg: float, width: int, height: int, seed: int) -> Image.Image:
        """
        Generate image using the loaded pipeline.

        Args:
            pipeline: Loaded diffusers pipeline
            prompt: Text prompt for generation
            negative_prompt: Negative prompt (optional)
            steps: Number of inference steps
            cfg: Guidance scale (classifier-free guidance strength)
            width: Image width (must be multiple of 8)
            height: Image height (must be multiple of 8)
            seed: Random seed for reproducibility

        Returns:
            PIL Image object

        Raises:
            Exception: If generation fails or is interrupted

        Based on verified API from FLUX examples:
        https://huggingface.co/docs/diffusers/main/api/pipelines/flux
        """
        print(f"[SDNQ Sampler] Generating image...")
        print(f"[SDNQ Sampler]   Prompt: {prompt[:100]}{'...' if len(prompt) > 100 else ''}")
        print(f"[SDNQ Sampler]   Steps: {steps}, CFG: {cfg}")
        print(f"[SDNQ Sampler]   Size: {width}x{height}")
        print(f"[SDNQ Sampler]   Seed: {seed}")

        # Check for interruption before starting
        if self.check_interrupted():
            raise InterruptedError("Generation interrupted by user")

        try:
            # Create generator for reproducible generation
            # Generator handles random sampling during denoising
            generator = torch.Generator(device="cuda").manual_seed(seed)

            # Build pipeline call kwargs
            # Only include parameters that are supported by the specific pipeline
            # Different pipelines have different signatures (FLUX.2 doesn't accept negative_prompt)
            pipeline_kwargs = {
                "prompt": prompt,
                "num_inference_steps": steps,
                "guidance_scale": cfg,
                "width": width,
                "height": height,
                "generator": generator,
            }

            # Only add negative_prompt if it's not empty
            # Will be automatically removed if pipeline doesn't support it
            if negative_prompt and negative_prompt.strip():
                pipeline_kwargs["negative_prompt"] = negative_prompt

            # Try calling pipeline with all parameters
            # If negative_prompt is unsupported, retry without it
            try:
                result = pipeline(**pipeline_kwargs)
            except TypeError as e:
                # Check if error is about negative_prompt parameter
                if "negative_prompt" in str(e) and "unexpected keyword argument" in str(e):
                    # Pipeline doesn't support negative_prompt (e.g., FLUX.2, FLUX-schnell)
                    print(f"[SDNQ Sampler] ⚠️  Pipeline {type(pipeline).__name__} doesn't support negative_prompt - skipping it")

                    # Remove negative_prompt and retry
                    if "negative_prompt" in pipeline_kwargs:
                        del pipeline_kwargs["negative_prompt"]

                    # Retry generation without negative_prompt
                    result = pipeline(**pipeline_kwargs)
                else:
                    # Different TypeError - re-raise with helpful message
                    import re
                    match = re.search(r"unexpected keyword argument '(\w+)'", str(e))
                    param_name = match.group(1) if match else "unknown"
                    raise Exception(
                        f"Pipeline doesn't support parameter: '{param_name}'\n\n"
                        f"Error: {str(e)}\n\n"
                        f"Pipeline type: {type(pipeline).__name__}\n"
                        f"This pipeline has a different signature than expected.\n\n"
                        f"Please report this issue on GitHub with the pipeline type above."
                    )

            # Check for interruption after generation
            if self.check_interrupted():
                raise InterruptedError("Generation interrupted by user")

            # Extract first image from results
            # result.images[0] is a PIL.Image.Image object
            image = result.images[0]

            print(f"[SDNQ Sampler] Image generated! Size: {image.size}")

            return image

        except InterruptedError:
            raise
        except Exception as e:
            # Don't double-wrap exceptions we already formatted
            if "Pipeline doesn't support parameter" in str(e):
                raise

            # Other errors - provide troubleshooting
            raise Exception(
                f"Failed to generate image\n\n"
                f"Error: {str(e)}\n\n"
                f"Troubleshooting:\n"
                f"1. Check VRAM usage (reduce size or use smaller model)\n"
                f"2. Verify parameters are valid (size multiple of 8, CFG reasonable)\n"
                f"3. Try reducing steps if running out of memory\n"
                f"4. Some models have specific parameter requirements (check HuggingFace page)\n"
                f"5. Look at the error message above for specific details"
            )

    def pil_to_comfy_tensor(self, pil_image: Image.Image) -> torch.Tensor:
        """
        Convert PIL Image to ComfyUI IMAGE tensor format.

        ComfyUI IMAGE format (verified from nodes.py LoadImage node):
        - Shape: [N, H, W, C] (batch, height, width, channels)
        - Dtype: torch.float32
        - Range: 0.0 to 1.0 (normalized)
        - Color: RGB

        Args:
            pil_image: PIL.Image.Image object

        Returns:
            torch.Tensor in ComfyUI format [1, H, W, 3]

        Based on verified conversion from ComfyUI nodes.py:
        https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py
        """
        # Ensure image is RGB (no alpha channel)
        pil_image = pil_image.convert("RGB")

        # Convert to numpy array and normalize to 0-1 range
        # PIL images are uint8 (0-255), ComfyUI uses float32 (0.0-1.0)
        numpy_image = np.array(pil_image).astype(np.float32) / 255.0

        # Convert to tensor and add batch dimension
        # [H, W, C] -> [1, H, W, C]
        tensor = torch.from_numpy(numpy_image)[None, :]

        print(f"[SDNQ Sampler] Converted to ComfyUI tensor: shape={tensor.shape}, dtype={tensor.dtype}")

        return tensor

    def generate(self, model_selection: str, custom_model_path: str, prompt: str,
                negative_prompt: str, steps: int, cfg: float, width: int, height: int,
                seed: int, scheduler: str, dtype: str, memory_mode: str, auto_download: bool,
                lora_selection: str = "[None]", lora_custom_path: str = "", lora_strength: float = 1.0) -> Tuple[torch.Tensor]:
        """
        Main generation function called by ComfyUI.

        This is the entry point when the node executes in a workflow.

        Args:
            model_selection: Selected model from dropdown
            custom_model_path: Custom model path (if [Custom Path] selected)
            prompt: Text prompt
            negative_prompt: Negative prompt
            steps: Inference steps
            cfg: Guidance scale
            width: Image width
            height: Image height
            seed: Random seed
            scheduler: Scheduler algorithm name
            dtype: Data type string
            memory_mode: Memory management mode ("gpu", "balanced", "lowvram")
            auto_download: Whether to auto-download models
            lora_selection: Selected LoRA from dropdown ([None], [Custom Path], or filename)
            lora_custom_path: Custom LoRA path (used when [Custom Path] selected)
            lora_strength: LoRA influence strength (-5.0 to +5.0)

        Returns:
            Tuple containing (IMAGE,) in ComfyUI format

        Raises:
            ValueError: For invalid inputs
            FileNotFoundError: For missing models/paths
            Exception: For other errors during loading/generation
        """
        print(f"\n{'='*60}")
        print(f"[SDNQ Sampler] Starting generation")
        print(f"{'='*60}\n")

        self.interrupted = False

        try:
            # Step 1: Load or download model
            model_path, was_downloaded = self.load_or_download_model(
                model_selection,
                custom_model_path,
                auto_download
            )

            # Step 2: Load pipeline (with caching)
            # Check if we need to reload the pipeline
            if (self.pipeline is None or
                self.current_model_path != model_path or
                self.current_dtype != dtype or
                self.current_memory_mode != memory_mode):

                print(f"[SDNQ Sampler] Pipeline cache miss - loading model...")
                self.pipeline = self.load_pipeline(model_path, dtype, memory_mode)
                self.current_model_path = model_path
                self.current_dtype = dtype
                self.current_memory_mode = memory_mode
                # Clear LoRA and scheduler cache when pipeline changes
                self.current_lora_path = None
                self.current_lora_strength = None
                self.current_scheduler = None
            else:
                print(f"[SDNQ Sampler] Using cached pipeline")

            # Step 2.5: Handle LoRA loading/unloading
            # Resolve actual LoRA path from lora_selection and lora_custom_path
            lora_path = None
            if lora_selection == "[None]":
                lora_path = None
            elif lora_selection == "[Custom Path]":
                lora_path = lora_custom_path if lora_custom_path and lora_custom_path.strip() else None
            else:
                # User selected a LoRA from the dropdown
                # Build full path from ComfyUI loras folder
                if COMFYUI_AVAILABLE:
                    try:
                        lora_folders = folder_paths.get_folder_paths("loras")
                        if lora_folders:
                            # Try to find the file in lora folders
                            for lora_folder in lora_folders:
                                potential_path = os.path.join(lora_folder, lora_selection)
                                if os.path.exists(potential_path):
                                    lora_path = potential_path
                                    break
                            if not lora_path:
                                # Fallback: use first folder + filename
                                lora_path = os.path.join(lora_folders[0], lora_selection)
                    except Exception as e:
                        print(f"[SDNQ Sampler] Warning: Could not resolve LoRA path: {e}")
                        lora_path = lora_selection  # Try using it as-is

            # Check if LoRA configuration has changed
            lora_changed = (lora_path != self.current_lora_path or
                           lora_strength != self.current_lora_strength)

            if lora_path and lora_path.strip():
                # User wants to use LoRA
                if lora_changed:
                    print(f"[SDNQ Sampler] LoRA configuration changed - updating...")

                    # Unload previous LoRA if it exists
                    if self.current_lora_path:
                        self.unload_lora(self.pipeline)

                    # Load new LoRA
                    self.load_lora(self.pipeline, lora_path, lora_strength)
                    self.current_lora_path = lora_path
                    self.current_lora_strength = lora_strength
                else:
                    print(f"[SDNQ Sampler] Using cached LoRA: {lora_path}")
            else:
                # User doesn't want LoRA, but we have one loaded
                if self.current_lora_path:
                    print(f"[SDNQ Sampler] Unloading LoRA...")
                    self.unload_lora(self.pipeline)
                    self.current_lora_path = None
                    self.current_lora_strength = None

            # Step 2.6: Handle scheduler swap
            # Check if scheduler changed from cached value
            if scheduler != self.current_scheduler:
                print(f"[SDNQ Sampler] Scheduler changed - swapping to {scheduler}...")
                self.swap_scheduler(self.pipeline, scheduler)
                self.current_scheduler = scheduler
            else:
                if self.current_scheduler:
                    print(f"[SDNQ Sampler] Using cached scheduler: {scheduler}")

            # Step 3: Generate image
            pil_image = self.generate_image(
                self.pipeline,
                prompt,
                negative_prompt,
                steps,
                cfg,
                width,
                height,
                seed
            )

            # Step 4: Convert to ComfyUI format
            comfy_tensor = self.pil_to_comfy_tensor(pil_image)

            print(f"\n{'='*60}")
            print(f"[SDNQ Sampler] Generation complete!")
            print(f"{'='*60}\n")

            # Return as tuple (ComfyUI expects tuple of outputs)
            return (comfy_tensor,)

        except InterruptedError as e:
            print(f"\n{'='*60}")
            print(f"[SDNQ Sampler] Generation interrupted")
            print(f"{'='*60}\n")
            raise

        except (ValueError, FileNotFoundError) as e:
            # User errors - display message clearly
            print(f"\n{'='*60}")
            print(f"[SDNQ Sampler] Error: {str(e)}")
            print(f"{'='*60}\n")
            raise

        except Exception as e:
            # Unexpected errors - provide full traceback
            print(f"\n{'='*60}")
            print(f"[SDNQ Sampler] Unexpected error occurred")
            print(f"{'='*60}")
            print(f"Error type: {type(e).__name__}")
            print(f"Error message: {str(e)}")
            print(f"\nFull traceback:")
            traceback.print_exc()
            print(f"{'='*60}\n")
            raise


# ============================================================================
# V1 API - Node Registration (ComfyUI Standard)
# ============================================================================

NODE_CLASS_MAPPINGS = {
    "SDNQSampler": SDNQSampler,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "SDNQSampler": "SDNQ Sampler",
}

# ============================================================================
# V3 API - Metadata (ComfyUI V3 Extension System)
# ============================================================================

# V3 API: Node metadata
__comfy_node_metadata__ = {
    "SDNQSampler": {
        "display_name": "SDNQ Sampler",
        "description": "Load SDNQ quantized models and generate images with 50-75% VRAM savings",
        "category": "sampling/SDNQ",
        "version": "1.0.0",
        "author": "ComfyUI-SDNQ Contributors",
        "license": "Apache-2.0",
        "tags": ["sampling", "sdnq", "quantization", "vram", "flux", "sd3"],
        "deprecated": False,
    }
}
